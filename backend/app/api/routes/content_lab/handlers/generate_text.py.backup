"""
Handler de generaci贸n de texto para Content Lab.
Filosof铆a: No velocity, only precision 
"""
from typing import Dict, Any
from fastapi import HTTPException
import logging

from app.api.routes.content_lab.builders.prompt_builder import (
    build_user_prompt, build_system_prompt
)
from app.services.ai_providers import AIProviders
from app.infrastructure.supabase_service import get_supabase_service
from app.infrastructure.repositories.client_context_repository import ClientContextRepository
from app.infrastructure.repositories.prompt_vault_repository import PromptVaultRepository

logger = logging.getLogger(__name__)


async def handle_generate_text(
    account_id: str,
    content_type: str,
    brief: str,
    language: str = "es",
    director: str = "REX"
) -> dict:
    """
    Handler HTTP para generaci贸n de texto.

    Workflow:
    1. Obtener client_id y contexto desde account_id
    2. Construir prompts (user + system)
    3. Llamar al AI provider seleccionado (multi-engine)
    4. Guardar resultado en DB
    5. Retornar response en formato flat

    Args:
        account_id: Social account UUID
        content_type: Tipo de contenido (caption, story, etc.)
        brief: Brief del usuario
        language: Idioma (default: es)
        director: AI Director (NOVA, ATLAS, LUNA, REX, VERA, KIRA, ORACLE)

    Returns:
        Dict con generated_text, content_type, provider, model, cached, tokens_used

    Raises:
        HTTPException: Si falla validaci贸n o generaci贸n
    """
    try:
        # Normalize content_type aliases (frontend may send variations)
        CONTENT_TYPE_MAP = {
            "reel_script": "reel",
            "reel_tiktok": "reel",
            "ad": "anuncio",
            "hashtag": "hashtags",
            "topic": "hashtags",  # Frontend sometimes sends "topic" for hashtags
        }
        content_type = CONTENT_TYPE_MAP.get(content_type, content_type)

        # Get Supabase client
        supabase = get_supabase_service()

        # 1. Obtener client_id - intenta social_accounts, luego clients
        account_response = supabase.client.table("social_accounts")\
            .select("client_id, platform, clients!inner(name, plan)")\
            .eq("id", account_id)\
            .execute()

        social_account_id = None
        if account_response.data:
            # Es un social_account
            account = account_response.data[0]
            client_id = account["client_id"]
            client_name = account["clients"]["name"]
            plan = account["clients"].get("plan") or "pro_197"
            platform = account["platform"]
            social_account_id = account_id
        else:
            # Intenta buscar como client_id directo
            client_response = supabase.client.table("clients")\
                .select("id, name, plan")\
                .eq("id", account_id)\
                .execute()
            if not client_response.data:
                raise HTTPException(404, f"Account or client {account_id} not found")
            client = client_response.data[0]
            client_id = client["id"]
            client_name = client["name"]
            plan = client.get("plan") or "pro_197"
            platform = "instagram"  # Default platform
            # Buscar primer social_account del cliente
            social_resp = supabase.client.table("social_accounts")\
                .select("id, platform")\
                .eq("client_id", client_id)\
                .limit(1)\
                .execute()
            if social_resp.data:
                social_account_id = social_resp.data[0]["id"]
                platform = social_resp.data[0]["platform"]
            else:
                raise HTTPException(400, f"Client {client_id} has no social accounts")

        # Normalize plan to match LLM_TIERS keys
        plan_map = {
            "basico": "basico_97",
            "pro": "pro_197",
            "enterprise": "enterprise_497",
            "basico_97": "basico_97",
            "pro_197": "pro_197",
            "enterprise_497": "enterprise_497"
        }
        user_tier = plan_map.get(plan, "pro_197")  # Default to pro_197

        logger.info(
            f"Generating {content_type} for {client_name} ({user_tier}) - "
            f"brief: {brief[:50]}..."
        )

        # 2. Load client context + brand_file from client_context table
        context_repo = ClientContextRepository(supabase)
        client_context = context_repo.find_by_client_id(client_id)

        # Load brand_file (JSONB) from client_context
        brand_file_response = supabase.client.table("client_context")\
            .select("brand_file, vertical")\
            .eq("client_id", client_id)\
            .execute()

        brand_file = {}
        if brand_file_response.data and brand_file_response.data[0].get("brand_file"):
            brand_file = brand_file_response.data[0]["brand_file"]
            logger.info(f"Loaded brand_file for client {client_id}")

        # Extract brand voice rules from brand_file
        brand_voice_data = brand_file.get("voice", {})
        brand_voice_rules = {
            "primary_tone": brand_voice_data.get("primary_tone", "professional"),
            "language_style": brand_voice_data.get("language_style", "semiformal"),
            "personality_traits": brand_voice_data.get("personality_traits", []),
            "emojis_allowed": brand_voice_data.get("emojis_allowed", False),
            "do": brand_file.get("do", []),
            "dont": brand_file.get("dont", [])
        }

        # Use context if available, merge with brand_file
        if client_context and client_context.has_context():
            audience = client_context.target_audience or "General"
            tone = brand_voice_rules["primary_tone"]  # Brand file overrides
            brand_voice = client_context.brand_voice
            keywords = client_context.content_themes or []
            context_data = {
                "business_type": brand_file_response.data[0].get("vertical") if brand_file_response.data else client_context.niche,
                "preferred_formats": client_context.preferred_formats,
                "brand_voice_rules": brand_voice_rules
            }
            logger.info(f"Using enriched context + brand voice for client {client_id}")
        else:
            # Use brand_file if available, otherwise defaults
            context_data = {
                "business_type": brand_file_response.data[0].get("vertical") if brand_file_response.data else "generic",
                "brand_voice_rules": brand_voice_rules
            }
            audience = "General"
            tone = brand_voice_rules["primary_tone"]
            brand_voice = None
            keywords = []
            logger.info(f"Using brand_file for client {client_id}")

        goal = "engagement"

        # NEW: Query Prompt Vault for optimal prompt
        vault_repo = PromptVaultRepository(supabase)
        vertical = context_data.get("business_type") or "generic"

        vault_prompt = await vault_repo.select_optimal_prompt(
            category=content_type,
            vertical=vertical,
            platform=platform,
            agent_code="RAFA"  # Content Lab uses RAFA for text generation
        )

        vault_used = None

        # 3. Construir prompts (use vault if available, otherwise default)
        if vault_prompt:
            # Use vault prompt template
            try:
                # Vault prompt_text can use placeholders: {brief}, {brand_voice}, etc.
                user_prompt = vault_prompt["prompt_text"].format(
                    brief=brief,
                    platform=platform,
                    audience=audience,
                    tone=tone,
                    client_name=client_name,
                    language=language,
                    goal=goal
                )

                vault_used = {
                    "id": vault_prompt["id"],
                    "name": vault_prompt["name"],
                    "technique": vault_prompt["technique"],
                    "performance_score": vault_prompt["performance_score"]
                }

                logger.info(
                    f"Using vault prompt '{vault_prompt['name']}' "
                    f"(technique={vault_prompt['technique']}, score={vault_prompt['performance_score']})"
                )

            except KeyError as e:
                logger.warning(
                    f"Vault prompt missing placeholder {e}, falling back to default builder"
                )
                user_prompt = build_user_prompt(
                    content_type=content_type,
                    brief=brief,
                    platform=platform,
                    audience=audience,
                    tone=tone,
                    goal=goal
                )
        else:
            # Fallback to default prompt builder
            user_prompt = build_user_prompt(
                content_type=content_type,
                brief=brief,
                platform=platform,
                audience=audience,
                tone=tone,
                goal=goal
            )
            logger.info("No vault prompt found, using default prompt builder")

        system_prompt = build_system_prompt(
            client_name=client_name,
            business_type=context_data.get("business_type"),
            brand_voice=brand_voice,
            keywords=keywords
        )

        # 4. Llamar al AI provider seleccionado (multi-engine)
        ai_providers = AIProviders()
        llm_response = await ai_providers.generate(
            director=director.upper(),
            prompt=user_prompt,
            system_prompt=system_prompt,
            max_tokens=2000,
            temperature=0.7
        )

        # 5. Guardar en DB (including vault_prompt_id for tracking)
        supabase.client.table("content_lab_generated").insert({
            "client_id": client_id,
            "social_account_id": social_account_id,
            "content_type": content_type,
            "content": llm_response["content"],
            "provider": llm_response["provider"],
            "model": llm_response["model"],
            "tokens_used": llm_response["tokens_used"],
            "vault_prompt_id": vault_used["id"] if vault_used else None
        }).execute()

        logger.info(
            f"Generated {content_type} for client {client_id} "
            f"via {director.upper()} ({llm_response['provider']}/{llm_response['model']})"
        )

        # 6. Retornar response en formato flat (with vault metadata)
        return {
            "generated_text": llm_response["content"],
            "content_type": content_type,
            "provider": llm_response["provider"],
            "model": llm_response["model"],
            "director": director.upper(),
            "cached": False,  # Not using cache for now
            "tokens_used": llm_response["tokens_used"],
            "vault_prompt_used": vault_used  # NEW: Include vault prompt metadata
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Text generation failed: {e}")
        raise HTTPException(500, f"Error generando contenido: {str(e)}")
